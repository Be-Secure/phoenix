{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals Ergonomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to support evals for three different cases:\n",
    "\n",
    "1. Running evals on an exported pandas dataframe and importing back into Phoenix,\n",
    "2. Running evals during LLM application execution via our callback system,\n",
    "3. Running evals post-hoc.\n",
    "\n",
    "Note: This notebook uses `phoenix.evals` everywhere for convenience, but we'll continue to keep `evals` in `experimental` for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running evals on an exported pandas dataframe and importing back into Phoenix\n",
    "\n",
    "The user is actively experimenting with their trace data in the form of a pandas dataframe. They wish to compute some evals, perhaps using our custom evaluators or perhaps using their own bespoke code, upload their evaluations, and see their evaluations reflected in Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import ClassificationPromptTemplate, LLMClassifier\n",
    "from phoenix.experimental.callbacks.langchain_tracer import OpenInferenceTracer\n",
    "\n",
    "session = px.launch_app()\n",
    "trace_df = session.export()\n",
    "tracer = OpenInferenceTracer()\n",
    "# Run your LLM application...\n",
    "\n",
    "trace_df = session.export_dataframe(\"span_kind == 'retriever'\")\n",
    "# massage the data...\n",
    "\n",
    "# compute relevance with a new prompt template\n",
    "model = OpenAIModel(model_name=\"gpt-4\")\n",
    "prompt_template = ClassificationPromptTemplate(\n",
    "    template=\"some prompt template the user is testing out with {context} and {question}\",\n",
    "    classes=[\"relevant\", \"irrelevant\"],\n",
    ")\n",
    "clf = LLMClassifier(model=model, prompt_template=prompt_template)\n",
    "trace_df[\"relevant\"] = clf.predict_dataframe(trace_df)\n",
    "\n",
    "# import back into phoenix\n",
    "# accepts pandas dataframe or pandas series indexed with the same span IDs\n",
    "session.import_evals(trace_df[\"relevant\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running evals during LLM application execution via our callback system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some users will trust our default templates, models, and configurations and won't need to dive deeper into the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import Evals\n",
    "from phoenix.experimental.callbacks.langchain_tracer import OpenInferenceTracer\n",
    "\n",
    "px.launch_app()\n",
    "tracer = OpenInferenceTracer(\n",
    "    evals=Evals.from_names([\"hallucination\", \"relevance\", \"toxicity\"]),\n",
    ")\n",
    "# define your chain...\n",
    "for query in queries:\n",
    "    chain.run(query, callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other users might want to configure their LLMs while still using our default templates. For example, their LLM application might running on open-source or fine-tuned models, but they want to use GPT-4 to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import Evals, JobConfig, OpenAIModel\n",
    "from phoenix.experimental.callbacks.langchain_tracer import OpenInferenceTracer\n",
    "\n",
    "px.launch_app()\n",
    "tracer = OpenInferenceTracer(\n",
    "    evals=Evals.from_names(\n",
    "        [\"hallucination\", \"relevance\", \"toxicity\"],\n",
    "        model=OpenAIModel(model_name=\"gpt-4\"),\n",
    "        job_config=JobConfig(max_requests_per_minute=200, max_tokens_per_minute=50000),\n",
    "    ),\n",
    ")\n",
    "# define your chain...\n",
    "for query in queries:\n",
    "    chain.run(query, callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should provide the ability for users to write their own custom evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import (\n",
    "    Evals,\n",
    "    Evaluator,\n",
    ")\n",
    "from phoenix.experimental.callbacks.langchain_tracer import OpenInferenceTracer\n",
    "\n",
    "px.launch_app()\n",
    "\n",
    "model = OpenAIModel(model_name=\"gpt-4\")\n",
    "prompt_template = ClassificationPromptTemplate(\n",
    "    template=\"some prompt template the user is testing out with {context} and {question}\",\n",
    "    classes=[\"relevant\", \"irrelevant\"],\n",
    ")\n",
    "clf = LLMClassifier(model=model, prompt_template=prompt_template)\n",
    "evaluator = Evaluator(clf)\n",
    "tracer = OpenInferenceTracer(\n",
    "    evals=Evals(evaluators=[evaluator]),\n",
    ")\n",
    "# Run your LLM application, evaluations appear in the Phoenix UI as the application runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- If someone is running our callbacks with one-click from LlamaIndex, how can they run with evals?\n",
    "- How does the user configure ranking metrics that are composite (i.e., require first that a classification is run and second that a score is computed)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running evals post-hoc\n",
    "\n",
    "Some users will want to run evals post-hoc an launch a phoenix dataset with their"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "spans = ...\n",
    "ds = px.TraceDataset.from_spans(spans)\n",
    "# define evals in the same manner as above\n",
    "evals = Evals(...)\n",
    "ds.run_evals(evals)\n",
    "px.launch_app(ds)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
